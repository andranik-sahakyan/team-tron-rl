{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import random, choice\n",
    "\n",
    "from matplotlib import cm\n",
    "from time import sleep\n",
    "from colosseumrl.envs.tron import TronGridEnvironment, TronRender, TronRllibEnvironment\n",
    "\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Dict, Discrete, Box\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG\n",
    "from ray.rllib.agents.dqn import DQNTrainer, DEFAULT_CONFIG\n",
    "\n",
    "from ray.rllib.models.preprocessors import Preprocessor\n",
    "from ray.rllib.models import ModelCatalog\n",
    "\n",
    "SEED = 1517\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Experimental\n",
    "It would most beneficial to have a fully dynamic training process where different agents train against each other and learn to communicate with and outsmart each other. Unfortunately, this process is highly unstable. We no longer have a \"static\" environment on which to train an agent on.\n",
    "\n",
    "## This is not stable and wont train well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TronExtractBoard(Preprocessor):\n",
    "    \"\"\" Wrapper to extract just the board from the game state and simplify it for the network. \"\"\"        \n",
    "    def _init_shape(self, obs_space, options):\n",
    "        board_size = env.observation_space['board'].shape[0]\n",
    "        return (board_size + 2, board_size + 2, 2)\n",
    "    \n",
    "    def transform(self, observation):\n",
    "        \n",
    "        # Pretty hacky way to get the current player number\n",
    "        # Requires having exactly 4 players\n",
    "        board = observation['board']\n",
    "        hor_offset = board.shape[0] // 2 + 2\n",
    "        top_player = board[1, hor_offset]\n",
    "        player_number = {1: 0, 4: 1, 3: 2, 2: 3}[top_player]\n",
    "        \n",
    "        return self._transform(observation, player_number)\n",
    "\n",
    "    def _transform(self, observation, rotate: int = 0):\n",
    "        board = observation['board'].copy()\n",
    "        \n",
    "        # Make all enemies look the same\n",
    "        board[board > 1] = -1\n",
    "        \n",
    "        # Mark where all of the player heads are\n",
    "        heads = np.zeros_like(board)\n",
    "        \n",
    "        if (rotate != 0):\n",
    "            heads.ravel()[observation['heads']] += 1 + ((observation['directions'] - rotate) % 4)\n",
    "            \n",
    "            board = np.rot90(board, k=rotate)\n",
    "            heads = np.rot90(heads, k=rotate)\n",
    "            \n",
    "        else:\n",
    "            heads.ravel()[observation['heads']] += 1 + observation['directions']\n",
    "            \n",
    "        # Pad the outsides so that we know where the wall is\n",
    "        board = np.pad(board, 1, 'constant', constant_values=-1)\n",
    "        heads = np.pad(heads, 1, 'constant', constant_values=-1)\n",
    "        \n",
    "        # Combine together\n",
    "        board = np.expand_dims(board, -1)\n",
    "        heads = np.expand_dims(heads, -1)\n",
    "        \n",
    "        return np.concatenate([board, heads], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training environment\n",
    "ray.init()\n",
    "\n",
    "def environment_creater(params=None):\n",
    "    return TronRllibEnvironment(board_size=13, num_players=4)\n",
    "\n",
    "env = environment_creater()\n",
    "tune.register_env(\"tron_multi_player\", environment_creater)\n",
    "ModelCatalog.register_custom_preprocessor(\"tron_prep\", TronExtractBoard)\n",
    "\n",
    "# Configure Deep Q Learning for multi-agent training\n",
    "config = DEFAULT_CONFIG.copy()\n",
    "config['num_workers'] = 4\n",
    "config[\"timesteps_per_iteration\"] = 128\n",
    "config['target_network_update_freq'] = 256\n",
    "config['buffer_size'] = 10_000\n",
    "config['schedule_max_timesteps'] = 100_000\n",
    "config['exploration_fraction'] = 0.9\n",
    "config['compress_observations'] = False\n",
    "config['num_envs_per_worker'] = 1\n",
    "config['train_batch_size'] = 256\n",
    "config['n_step'] = 2\n",
    "\n",
    "# All of the models will use the same network as before\n",
    "agent_config = {\n",
    "    \"model\": {\n",
    "        \"vf_share_layers\": True,\n",
    "        \"conv_filters\": [(64, 5, 2), (128, 3, 2), (256, 3, 2)],\n",
    "        \"fcnet_hiddens\": [128],\n",
    "        \"custom_preprocessor\": 'tron_prep'\n",
    "    }\n",
    "}\n",
    "\n",
    "# This time we have 4 seperate policies\n",
    "# All training AT THE SAME TIME\n",
    "config['multiagent'] = {\n",
    "        \"policy_mapping_fn\": lambda x: str(x),\n",
    "        \"policies\": {str(i): (None, env.observation_space, env.action_space, agent_config) for i in range(env.env.num_players)}\n",
    "}\n",
    "       \n",
    "trainer = DQNTrainer(config, \"tron_multi_player\")\n",
    "\n",
    "num_epoch = 300\n",
    "test_epochs = 1\n",
    "for epoch in range(num_epoch):\n",
    "    print(\"Training iteration: {}\".format(epoch), end='')\n",
    "    res = trainer.train()\n",
    "    print(f\", Average reward: {res['policy_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
