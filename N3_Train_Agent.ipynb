{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Option\n",
    "LOAD_FROM_CHECKPOINT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import random, choice\n",
    "\n",
    "from matplotlib import cm\n",
    "from time import sleep\n",
    "from colosseumrl.envs.tron import TronGridEnvironment, TronRender, TronRllibEnvironment\n",
    "\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Dict, Discrete, Box\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG\n",
    "from ray.rllib.agents.dqn import DQNTrainer, DEFAULT_CONFIG\n",
    "\n",
    "from ray.rllib.models.preprocessors import Preprocessor\n",
    "from ray.rllib.models import ModelCatalog\n",
    "\n",
    "SEED = 1517\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an Agent\n",
    "\n",
    "##### Thinking of a more intelligent agent is pretty hard. So let's make machine learning find one for us! First, let's train an agent to defeat our personal atempt. We will employ Rllib in order to train an agent using Deep Q-Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our manual agent again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class SimpleAvoidAgent:\n",
    "    \"\"\" Basic single player agent to test single player version of Tron. \"\"\"\n",
    "    def __init__(self, noise=0.1):\n",
    "        self.noise = noise\n",
    "\n",
    "    def __call__(self, env, observation):\n",
    "        # With some probability, select a random action for variation\n",
    "        if random() <= self.noise:\n",
    "            return choice([0, 1, 2])\n",
    "        \n",
    "        # Get game information\n",
    "        board = observation['board']\n",
    "        head = observation['heads'][0]\n",
    "        direction = observation['directions'][0]\n",
    "        \n",
    "        # Find the head of our body\n",
    "        board_size = board.shape[0]\n",
    "        x, y = head % board_size, head // board_size\n",
    "\n",
    "        # Check ahead. If it's clear, then take a step forward.\n",
    "        nx, ny = env.next_cell(x, y, direction, board_size)\n",
    "        if board[ny, nx] == 0:\n",
    "            return 0\n",
    "\n",
    "        # Check a random direction. If it's clear, then go there.\n",
    "        offset, action, backup = choice([(1, 1, 2), (-1, 2, 1)])\n",
    "        nx, ny = env.next_cell(x, y, (direction + offset) % 4, board_size)\n",
    "        if board[ny, nx] == 0:\n",
    "            return action\n",
    "\n",
    "        # Otherwise, go the opposite direction.\n",
    "        return backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Player Tron\n",
    "##### We create a simpler variant of tron featuring only one actively participating agent. This will simplify the RL task to training an agent to play against a fixed set of opponents. We can imagine this as embedding our manual agents within the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class SinglePlayer(gym.Env):\n",
    "    \"\"\" Transform tron into a single player game with predefined enemy agents. \"\"\"\n",
    "    def __init__(self, env, active_player = '0', agents = SimpleAvoidAgent()):       \n",
    "        if not isinstance(agents, list):\n",
    "            agents = [agents]\n",
    "        \n",
    "        self.agents = agents\n",
    "        self.active_player = active_player\n",
    "        self.env = env\n",
    "        \n",
    "        self.observation_space = env.observation_space\n",
    "        self.action_space = env.action_space\n",
    "        \n",
    "        self.observations = None\n",
    "        \n",
    "    def reset(self):\n",
    "        self.observations = self.env.reset()\n",
    "        return self.observations[self.active_player]\n",
    "        \n",
    "    def step(self, action, agents = None):\n",
    "        if agents is None:\n",
    "            agents = self.agents\n",
    "        \n",
    "        num_agents = len(agents)\n",
    "        actions = {}\n",
    "        \n",
    "        agent_id = 0\n",
    "        for player in self.env.players:\n",
    "            player = str(player)\n",
    "            \n",
    "            if player == self.active_player:\n",
    "                actions[player] = action\n",
    "            else:\n",
    "                actions[player] = agents[agent_id](self.env.env, self.observations[player])\n",
    "                agent_id  = (agent_id + 1) % num_agents\n",
    "        \n",
    "        self.observations, rewards, dones, info = self.env.step(actions)\n",
    "        \n",
    "        return self.observations[self.active_player], rewards[self.active_player], dones[self.active_player], info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation Preprocessing\n",
    "##### Often times the original form of the observation is not ideal for neural network input. Therefore, we have to pre-process the observation to extract the key bits of information so that the network can easily learn a value or policy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class TronExtractBoard(Preprocessor):\n",
    "    \"\"\" Wrapper to extract just the board from the game state and simplify it for the network. \"\"\"        \n",
    "    def _init_shape(self, obs_space, options):\n",
    "        board_size = env.observation_space['board'].shape[0]\n",
    "        return (board_size + 2, board_size + 2, 2)\n",
    "    \n",
    "    def transform(self, observation):\n",
    "        if 'board' in observation:\n",
    "            return self._transform(observation)\n",
    "        else:\n",
    "            return {player: self._transform(obs, int(player)) for player, obs in observation.items()}\n",
    "\n",
    "    def _transform(self, observation, rotate: int = 0):\n",
    "        board = observation['board'].copy()\n",
    "        \n",
    "        # Make all enemies look the same\n",
    "        board[board > 1] = -1\n",
    "        \n",
    "        # Mark where all of the player heads are\n",
    "        heads = np.zeros_like(board)\n",
    "        \n",
    "        if (rotate != 0):\n",
    "            heads.ravel()[observation['heads']] += 1 + ((observation['directions'] - rotate) % 4)\n",
    "            \n",
    "            board = np.rot90(board, k=rotate)\n",
    "            heads = np.rot90(heads, k=rotate)\n",
    "            \n",
    "        else:\n",
    "            heads.ravel()[observation['heads']] += 1 + observation['directions']\n",
    "            \n",
    "        # Pad the outsides so that we know where the wall is\n",
    "        board = np.pad(board, 1, 'constant', constant_values=-1)\n",
    "        heads = np.pad(heads, 1, 'constant', constant_values=-1)\n",
    "        \n",
    "        # Combine together\n",
    "        board = np.expand_dims(board, -1)\n",
    "        heads = np.expand_dims(heads, -1)\n",
    "        \n",
    "        return np.concatenate([board, heads], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def test(render, env, trainer, frame_time = 0.4):\n",
    "    policy = trainer.get_policy()\n",
    "    policy.cur_epsilon_value = 0\n",
    "    render.close()\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    action = None\n",
    "    reward = None\n",
    "    cumulative_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = trainer.compute_action(state, prev_action=action, prev_reward=reward)\n",
    "\n",
    "        state, reward, done, results = env.step(action)\n",
    "        cumulative_reward += reward\n",
    "        render.render(env.env.state)\n",
    "\n",
    "        sleep(frame_time)\n",
    "\n",
    "    render.render(env.env.state)    \n",
    "    return cumulative_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-09 23:55:48,951\tWARNING services.py:592 -- setpgrp failed, processes may not be cleaned up properly: [Errno 1] Operation not permitted.\n",
      "2020-03-09 23:55:48,972\tINFO resource_spec.py:212 -- Starting Ray with 4.88 GiB memory available for workers and up to 2.46 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-03-09 23:55:50,092\tINFO services.py:1093 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n",
      "2020-03-09 23:55:54,381\tINFO trainer.py:377 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "2020-03-09 23:55:54,432\tINFO trainer.py:524 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2020-03-09 23:55:54,474\tWARNING catalog.py:390 -- DeprecationWarning: Custom preprocessors are deprecated, since they sometimes conflict with the built-in preprocessors for handling complex observation spaces. Please use wrapper classes around your environment instead of preprocessors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2702)\u001b[0m E0309 23:55:54.352404900    2702 socket_utils_common_posix.cc:208] check for SO_REUSEPORT: {\"created\":\"@1583823354.352389200\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":185,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[36m(pid=2702)\u001b[0m E0309 23:55:54.352686000    2702 socket_utils_common_posix.cc:313] setsockopt(TCP_USER_TIMEOUT) Protocol not available\n",
      "\u001b[2m\u001b[36m(pid=2704)\u001b[0m E0309 23:55:54.380412700    2704 socket_utils_common_posix.cc:208] check for SO_REUSEPORT: {\"created\":\"@1583823354.380393000\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":185,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[36m(pid=2704)\u001b[0m E0309 23:55:54.380723200    2704 socket_utils_common_posix.cc:313] setsockopt(TCP_USER_TIMEOUT) Protocol not available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-09 23:55:58,692\tWARNING util.py:41 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2703)\u001b[0m E0309 23:55:54.482307000    2703 socket_utils_common_posix.cc:208] check for SO_REUSEPORT: {\"created\":\"@1583823354.482287000\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":185,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[36m(pid=2703)\u001b[0m E0309 23:55:54.482637600    2703 socket_utils_common_posix.cc:313] setsockopt(TCP_USER_TIMEOUT) Protocol not available\n",
      "\u001b[2m\u001b[36m(pid=2705)\u001b[0m E0309 23:55:54.596358100    2705 socket_utils_common_posix.cc:208] check for SO_REUSEPORT: {\"created\":\"@1583823354.596337700\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":185,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[36m(pid=2705)\u001b[0m E0309 23:55:54.596695600    2705 socket_utils_common_posix.cc:313] setsockopt(TCP_USER_TIMEOUT) Protocol not available\n",
      "Training iteration: 0\u001b[2m\u001b[36m(pid=2704)\u001b[0m 2020-03-09 23:56:09,159\tWARNING catalog.py:390 -- DeprecationWarning: Custom preprocessors are deprecated, since they sometimes conflict with the built-in preprocessors for handling complex observation spaces. Please use wrapper classes around your environment instead of preprocessors.\n",
      "\u001b[2m\u001b[36m(pid=2703)\u001b[0m 2020-03-09 23:56:09,134\tWARNING catalog.py:390 -- DeprecationWarning: Custom preprocessors are deprecated, since they sometimes conflict with the built-in preprocessors for handling complex observation spaces. Please use wrapper classes around your environment instead of preprocessors.\n",
      "\u001b[2m\u001b[36m(pid=2705)\u001b[0m 2020-03-09 23:56:09,165\tWARNING catalog.py:390 -- DeprecationWarning: Custom preprocessors are deprecated, since they sometimes conflict with the built-in preprocessors for handling complex observation spaces. Please use wrapper classes around your environment instead of preprocessors.\n",
      "\u001b[2m\u001b[36m(pid=2702)\u001b[0m 2020-03-09 23:56:09,316\tWARNING catalog.py:390 -- DeprecationWarning: Custom preprocessors are deprecated, since they sometimes conflict with the built-in preprocessors for handling complex observation spaces. Please use wrapper classes around your environment instead of preprocessors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andranik/anaconda3/envs/colosseumrl/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/andranik/anaconda3/envs/colosseumrl/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", Average reward: 3.066666666666667\n",
      "checkpoint saved at /home/andranik/ray_results/DQN_tron_single_player_2020-03-09_23-55-54_wowtdvl/checkpoint_1/checkpoint-1\n",
      "Training iteration: 1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", Average reward: 5.693069306930693\n",
      "Training iteration: 2, Average reward: 5.01\n",
      "Training iteration: 3, Average reward: 4.64\n",
      "Training iteration: 4, Average reward: 4.93\n",
      "Training iteration: 5, Average reward: 5.03\n",
      "Training iteration: 6, Average reward: 4.94\n",
      "Training iteration: 7, Average reward: 5.32\n",
      "Training iteration: 8, Average reward: 5.36\n",
      "Training iteration: 9, Average reward: 5.76\n",
      "Training iteration: 10, Average reward: 5.01\n",
      "Training iteration: 11, Average reward: 4.78\n",
      "Training iteration: 12, Average reward: 5.25\n",
      "Training iteration: 13, Average reward: 5.03\n",
      "Training iteration: 14, Average reward: 5.13\n",
      "Training iteration: 15, Average reward: 5.02\n",
      "Training iteration: 16, Average reward: 4.75\n",
      "Training iteration: 17, Average reward: 5.31\n",
      "Training iteration: 18, Average reward: 5.64\n",
      "Training iteration: 19, Average reward: 5.73\n",
      "Training iteration: 20, Average reward: 5.1\n",
      "Training iteration: 21, Average reward: 4.93\n",
      "Training iteration: 22, Average reward: 4.92\n",
      "Training iteration: 23, Average reward: 4.86\n",
      "Training iteration: 24, Average reward: 4.73\n",
      "Training iteration: 25, Average reward: 4.46\n",
      "Training iteration: 26, Average reward: 4.79\n",
      "Training iteration: 27, Average reward: 5.95\n",
      "Training iteration: 28, Average reward: 6.3\n",
      "Training iteration: 29, Average reward: 6.06\n",
      "Training iteration: 30, Average reward: 5.02\n",
      "Training iteration: 31, Average reward: 5.55\n",
      "Training iteration: 32, Average reward: 5.96\n",
      "Training iteration: 33, Average reward: 5.47\n",
      "Training iteration: 34, Average reward: 5.49\n",
      "Training iteration: 35, Average reward: 5.47\n",
      "Training iteration: 36, Average reward: 5.57\n",
      "Training iteration: 37, Average reward: 5.61\n",
      "Training iteration: 38, Average reward: 5.88\n",
      "Training iteration: 39, Average reward: 5.59\n",
      "Training iteration: 40, Average reward: 5.99\n",
      "Training iteration: 41, Average reward: 6.31\n",
      "Training iteration: 42, Average reward: 6.22\n",
      "Training iteration: 43, Average reward: 6.61\n",
      "Training iteration: 44, Average reward: 6.99\n",
      "Training iteration: 45, Average reward: 6.09\n",
      "Training iteration: 46, Average reward: 5.88\n",
      "Training iteration: 47, Average reward: 6.47\n",
      "Training iteration: 48, Average reward: 6.93\n",
      "Training iteration: 49, Average reward: 6.26\n",
      "Training iteration: 50, Average reward: 5.97\n",
      "Training iteration: 51, Average reward: 5.53\n",
      "Training iteration: 52, Average reward: 5.82\n",
      "Training iteration: 53, Average reward: 6.0\n",
      "Training iteration: 54, Average reward: 6.21\n",
      "Training iteration: 55, Average reward: 6.28\n",
      "Training iteration: 56, Average reward: 6.2\n",
      "Training iteration: 57, Average reward: 6.97\n",
      "Training iteration: 58, Average reward: 6.93\n",
      "Training iteration: 59, Average reward: 6.67\n",
      "Training iteration: 60, Average reward: 6.03\n",
      "Training iteration: 61, Average reward: 5.26\n",
      "Training iteration: 62, Average reward: 5.2\n",
      "Training iteration: 63, Average reward: 5.96\n",
      "Training iteration: 64, Average reward: 6.1\n",
      "Training iteration: 65, Average reward: 6.53\n",
      "Training iteration: 66, Average reward: 6.98\n",
      "Training iteration: 67, Average reward: 7.18\n",
      "Training iteration: 68, Average reward: 7.0\n",
      "Training iteration: 69, Average reward: 6.67\n",
      "Training iteration: 70, Average reward: 6.5\n",
      "Training iteration: 71, Average reward: 6.01\n",
      "Training iteration: 72, Average reward: 6.33\n",
      "Training iteration: 73, Average reward: 6.19\n",
      "Training iteration: 74, Average reward: 5.97\n",
      "Training iteration: 75, Average reward: 6.37\n",
      "Training iteration: 76, Average reward: 6.25\n",
      "Training iteration: 77, Average reward: 6.64\n",
      "Training iteration: 78, Average reward: 6.46\n",
      "Training iteration: 79, Average reward: 7.75\n",
      "Training iteration: 80, Average reward: 8.15\n",
      "Training iteration: 81, Average reward: 8.19\n",
      "Training iteration: 82, Average reward: 6.46\n",
      "Training iteration: 83, Average reward: 6.62\n",
      "Training iteration: 84, Average reward: 7.89\n",
      "Training iteration: 85, Average reward: 7.73\n",
      "Training iteration: 86, Average reward: 6.41\n",
      "Training iteration: 87, Average reward: 6.62\n",
      "Training iteration: 88, Average reward: 6.66\n",
      "Training iteration: 89, Average reward: 6.85\n",
      "Training iteration: 90, Average reward: 7.03\n",
      "Training iteration: 91, Average reward: 6.27\n",
      "Training iteration: 92, Average reward: 7.11\n",
      "Training iteration: 93, Average reward: 7.42\n",
      "Training iteration: 94, Average reward: 8.03\n",
      "Training iteration: 95, Average reward: 7.98\n",
      "Training iteration: 96, Average reward: 7.3\n",
      "Training iteration: 97, Average reward: 6.96\n",
      "Training iteration: 98, Average reward: 6.82\n",
      "Training iteration: 99, Average reward: 6.99\n",
      "Training iteration: 100, Average reward: 7.0\n",
      "checkpoint saved at /home/andranik/ray_results/DQN_tron_single_player_2020-03-09_23-55-54_wowtdvl/checkpoint_101/checkpoint-101\n",
      "Training iteration: 101, Average reward: 7.21\n",
      "Training iteration: 102, Average reward: 6.99\n",
      "Training iteration: 103, Average reward: 7.0\n",
      "Training iteration: 104, Average reward: 7.45\n",
      "Training iteration: 105, Average reward: 7.85\n",
      "Training iteration: 106, Average reward: 7.66\n",
      "Training iteration: 107, Average reward: 6.89\n",
      "Training iteration: 108, Average reward: 6.57\n",
      "Training iteration: 109, Average reward: 7.03\n",
      "Training iteration: 110, Average reward: 7.55\n",
      "Training iteration: 111, Average reward: 8.84\n",
      "Training iteration: 112, Average reward: 9.44\n",
      "Training iteration: 113, Average reward: 7.69\n",
      "Training iteration: 114, Average reward: 7.73\n",
      "Training iteration: 115, Average reward: 8.02\n",
      "Training iteration: 116, Average reward: 7.18\n",
      "Training iteration: 117, Average reward: 7.03\n",
      "Training iteration: 118, Average reward: 7.87\n",
      "Training iteration: 119, Average reward: 7.77\n",
      "Training iteration: 120, Average reward: 7.08\n",
      "Training iteration: 121, Average reward: 7.31\n",
      "Training iteration: 122, Average reward: 7.47\n",
      "Training iteration: 123, Average reward: 7.74\n",
      "Training iteration: 124, Average reward: 8.19\n",
      "Training iteration: 125, Average reward: 8.31\n",
      "Training iteration: 126, Average reward: 8.2\n",
      "Training iteration: 127, Average reward: 7.91\n",
      "Training iteration: 128, Average reward: 7.82\n",
      "Training iteration: 129, Average reward: 7.84\n",
      "Training iteration: 130, Average reward: 7.74\n",
      "Training iteration: 131, Average reward: 7.58\n",
      "Training iteration: 132, Average reward: 8.3\n",
      "Training iteration: 133, Average reward: 8.39\n",
      "Training iteration: 134, Average reward: 8.81\n",
      "Training iteration: 135, Average reward: 8.14\n",
      "Training iteration: 136, Average reward: 8.34\n",
      "Training iteration: 137, Average reward: 7.97\n",
      "Training iteration: 138, Average reward: 8.09\n",
      "Training iteration: 139, Average reward: 8.34\n",
      "Training iteration: 140, Average reward: 8.52\n",
      "Training iteration: 141, Average reward: 8.85\n",
      "Training iteration: 142, Average reward: 9.16\n",
      "Training iteration: 143, Average reward: 8.24\n",
      "Training iteration: 144, Average reward: 8.56\n",
      "Training iteration: 145, Average reward: 8.39\n",
      "Training iteration: 146, Average reward: 8.92\n",
      "Training iteration: 147, Average reward: 8.71\n",
      "Training iteration: 148, Average reward: 8.29\n",
      "Training iteration: 149, Average reward: 8.36\n",
      "Training iteration: 150, Average reward: 8.47\n",
      "Training iteration: 151, Average reward: 8.87\n",
      "Training iteration: 152, Average reward: 9.51\n",
      "Training iteration: 153, Average reward: 9.61\n",
      "Training iteration: 154, Average reward: 10.27\n",
      "Training iteration: 155, Average reward: 10.26\n",
      "Training iteration: 156, Average reward: 10.12\n",
      "Training iteration: 157, Average reward: 8.65\n",
      "Training iteration: 158, Average reward: 8.79\n",
      "Training iteration: 159, Average reward: 8.84\n",
      "Training iteration: 160, Average reward: 9.49\n",
      "Training iteration: 161, Average reward: 9.54\n",
      "Training iteration: 162, Average reward: 10.92\n",
      "Training iteration: 163, Average reward: 10.36\n",
      "Training iteration: 164, Average reward: 9.8\n",
      "Training iteration: 165, Average reward: 9.33\n",
      "Training iteration: 166, Average reward: 9.12\n",
      "Training iteration: 167, Average reward: 8.39\n",
      "Training iteration: 168, Average reward: 8.45\n",
      "Training iteration: 169, Average reward: 8.85\n",
      "Training iteration: 170, Average reward: 10.34\n",
      "Training iteration: 171, Average reward: 10.34\n",
      "Training iteration: 172, Average reward: 11.03\n",
      "Training iteration: 173, Average reward: 11.13\n",
      "Training iteration: 174, Average reward: 11.24\n",
      "Training iteration: 175, Average reward: 11.22\n",
      "Training iteration: 176, Average reward: 10.88\n",
      "Training iteration: 177, Average reward: 9.38\n",
      "Training iteration: 178, Average reward: 9.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 179, Average reward: 9.22\n",
      "Training iteration: 180, Average reward: 10.4\n",
      "Training iteration: 181, Average reward: 10.02\n",
      "Training iteration: 182, Average reward: 10.3\n",
      "Training iteration: 183, Average reward: 9.62\n",
      "Training iteration: 184, Average reward: 9.78\n",
      "Training iteration: 185, Average reward: 9.72\n",
      "Training iteration: 186, Average reward: 9.53\n",
      "Training iteration: 187, Average reward: 10.6\n",
      "Training iteration: 188, Average reward: 11.14\n",
      "Training iteration: 189, Average reward: 10.83\n",
      "Training iteration: 190, Average reward: 10.92\n",
      "Training iteration: 191, Average reward: 10.93\n",
      "Training iteration: 192, Average reward: 11.82\n",
      "Training iteration: 193, Average reward: 11.69\n",
      "Training iteration: 194, Average reward: 12.04\n",
      "Training iteration: 195, Average reward: 10.73\n",
      "Training iteration: 196, Average reward: 11.21\n",
      "Training iteration: 197, Average reward: 11.37\n",
      "Training iteration: 198, Average reward: 10.92\n",
      "Training iteration: 199, Average reward: 10.64\n",
      "Training iteration: 200, Average reward: 10.81\n",
      "checkpoint saved at /home/andranik/ray_results/DQN_tron_single_player_2020-03-09_23-55-54_wowtdvl/checkpoint_201/checkpoint-201\n",
      "Training iteration: 201, Average reward: 13.37\n",
      "Training iteration: 202, Average reward: 15.6\n",
      "Training iteration: 203, Average reward: 14.24\n",
      "Training iteration: 204, Average reward: 16.62\n",
      "Training iteration: 205, Average reward: 13.15\n",
      "Training iteration: 206, Average reward: 12.78\n",
      "Training iteration: 207, Average reward: 11.76\n",
      "Training iteration: 208, Average reward: 11.75\n",
      "Training iteration: 209, Average reward: 11.31\n",
      "Training iteration: 210, Average reward: 12.49\n",
      "Training iteration: 211, Average reward: 13.43\n",
      "Training iteration: 212, Average reward: 12.33\n",
      "Training iteration: 213, Average reward: 10.08\n",
      "Training iteration: 214, Average reward: 10.58\n",
      "Training iteration: 215, Average reward: 11.17\n",
      "Training iteration: 216, Average reward: 12.9\n",
      "Training iteration: 217, Average reward: 13.15\n",
      "Training iteration: 218, Average reward: 12.75\n",
      "Training iteration: 219, Average reward: 12.03\n",
      "Training iteration: 220, Average reward: 10.85\n",
      "Training iteration: 221, Average reward: 12.05\n",
      "Training iteration: 222, Average reward: 14.39\n",
      "Training iteration: 223, Average reward: 17.02\n",
      "Training iteration: 224, Average reward: 17.1\n",
      "Training iteration: 225, Average reward: 19.7\n",
      "Training iteration: 226, Average reward: 19.29\n",
      "Training iteration: 227, Average reward: 16.34\n",
      "Training iteration: 228, Average reward: 15.83\n",
      "Training iteration: 229, Average reward: 14.64\n",
      "Training iteration: 230, Average reward: 14.36\n",
      "Training iteration: 231, Average reward: 15.21\n",
      "Training iteration: 232, Average reward: 19.26\n",
      "Training iteration: 233, Average reward: 17.67\n",
      "Training iteration: 234, Average reward: 18.26\n",
      "Training iteration: 235, Average reward: 18.7\n",
      "Training iteration: 236, Average reward: 15.72\n",
      "Training iteration: 237, Average reward: 17.05\n",
      "Training iteration: 238, Average reward: 16.39\n",
      "Training iteration: 239, Average reward: 16.01\n",
      "Training iteration: 240, Average reward: 17.2\n",
      "Training iteration: 241, Average reward: 15.99\n",
      "Training iteration: 242, Average reward: 16.93\n",
      "Training iteration: 243, Average reward: 16.35\n",
      "Training iteration: 244, Average reward: 16.14\n",
      "Training iteration: 245, Average reward: 16.27\n",
      "Training iteration: 246, Average reward: 15.82\n",
      "Training iteration: 247, Average reward: 16.21\n",
      "Training iteration: 248, Average reward: 18.13\n",
      "Training iteration: 249, Average reward: 18.01\n",
      "Training iteration: 250, Average reward: 20.35\n",
      "Training iteration: 251, Average reward: 23.38\n",
      "Training iteration: 252, Average reward: 20.97\n",
      "Training iteration: 253, Average reward: 22.05\n",
      "Training iteration: 254, Average reward: 22.31\n",
      "Training iteration: 255, Average reward: 21.96\n",
      "Training iteration: 256, Average reward: 25.0\n",
      "Training iteration: 257, Average reward: 27.23\n",
      "Training iteration: 258, Average reward: 28.82\n",
      "Training iteration: 259, Average reward: 30.45\n",
      "Training iteration: 260, Average reward: 31.15\n",
      "Training iteration: 261, Average reward: 29.38\n",
      "Training iteration: 262, Average reward: 26.6\n",
      "Training iteration: 263, Average reward: 24.3\n",
      "Training iteration: 264, Average reward: 20.0\n",
      "Training iteration: 265, Average reward: 19.43\n",
      "Training iteration: 266, Average reward: 19.74\n",
      "Training iteration: 267, Average reward: 24.1\n",
      "Training iteration: 268, Average reward: 28.29\n",
      "Training iteration: 269, Average reward: 25.69\n",
      "Training iteration: 270, Average reward: 26.04\n",
      "Training iteration: 271, Average reward: 24.64\n",
      "Training iteration: 272, Average reward: 23.13\n",
      "Training iteration: 273, Average reward: 21.57\n",
      "Training iteration: 274, Average reward: 23.21\n",
      "Training iteration: 275, Average reward: 25.36\n",
      "Training iteration: 276, Average reward: 24.1\n",
      "Training iteration: 277, Average reward: 27.6\n",
      "Training iteration: 278, Average reward: 26.78\n",
      "Training iteration: 279, Average reward: 32.89\n",
      "Training iteration: 280, Average reward: 33.13\n",
      "Training iteration: 281, Average reward: 35.15\n",
      "Training iteration: 282, Average reward: 40.02\n",
      "Training iteration: 283, Average reward: 36.16\n",
      "Training iteration: 284, Average reward: 23.25\n",
      "Training iteration: 285, Average reward: 12.28\n",
      "Training iteration: 286, Average reward: 10.95\n",
      "Training iteration: 287, Average reward: 11.73\n",
      "Training iteration: 288, Average reward: 17.2\n",
      "Training iteration: 289, Average reward: 20.2\n",
      "Training iteration: 290, Average reward: 21.86\n",
      "Training iteration: 291, Average reward: 28.27\n",
      "Training iteration: 292, Average reward: 29.73\n",
      "Training iteration: 293, Average reward: 24.16\n",
      "Training iteration: 294, Average reward: 23.39\n",
      "Training iteration: 295, Average reward: 17.63\n",
      "Training iteration: 296, Average reward: 16.84\n",
      "Training iteration: 297, Average reward: 16.84\n",
      "Training iteration: 298, Average reward: 16.46\n",
      "Training iteration: 299, Average reward: 18.87\n",
      "Training iteration: 300, Average reward: 22.05\n",
      "checkpoint saved at /home/andranik/ray_results/DQN_tron_single_player_2020-03-09_23-55-54_wowtdvl/checkpoint_301/checkpoint-301\n",
      "Training iteration: 301, Average reward: 23.62\n",
      "Training iteration: 302, Average reward: 24.44\n",
      "Training iteration: 303, Average reward: 25.3\n",
      "Training iteration: 304, Average reward: 24.21\n",
      "Training iteration: 305, Average reward: 24.1\n",
      "Training iteration: 306, Average reward: 29.86\n",
      "Training iteration: 307, Average reward: 28.41\n",
      "Training iteration: 308, Average reward: 33.01\n",
      "Training iteration: 309, Average reward: 30.38\n",
      "Training iteration: 310, Average reward: 8.54\n",
      "Training iteration: 311, Average reward: 3.61\n",
      "Training iteration: 312, Average reward: 3.86\n",
      "Training iteration: 313, Average reward: 10.52\n",
      "Training iteration: 314, Average reward: 18.98\n",
      "Training iteration: 315, Average reward: 28.4\n",
      "Training iteration: 316, Average reward: 31.9\n",
      "Training iteration: 317, Average reward: 36.75\n",
      "Training iteration: 318, Average reward: 35.8\n",
      "Training iteration: 319, Average reward: 31.28\n",
      "Training iteration: 320, Average reward: 31.3\n",
      "Training iteration: 321, Average reward: 36.63\n",
      "Training iteration: 322, Average reward: 42.41\n",
      "Training iteration: 323, Average reward: 45.34\n",
      "Training iteration: 324, Average reward: 45.16\n",
      "Training iteration: 325, Average reward: 46.79\n",
      "Training iteration: 326, Average reward: 44.58\n",
      "Training iteration: 327, Average reward: 43.34\n",
      "Training iteration: 328, Average reward: 43.15\n",
      "Training iteration: 329, Average reward: 50.07\n",
      "Training iteration: 330, Average reward: 47.87\n",
      "Training iteration: 331, Average reward: 50.19\n",
      "Training iteration: 332, Average reward: 47.2\n",
      "Training iteration: 333, Average reward: 38.73\n",
      "Training iteration: 334, Average reward: 41.61\n",
      "Training iteration: 335, Average reward: 38.55\n",
      "Training iteration: 336, Average reward: 33.09\n",
      "Training iteration: 337, Average reward: 30.64\n",
      "Training iteration: 338, Average reward: 30.75\n",
      "Training iteration: 339, Average reward: 34.27\n",
      "Training iteration: 340, Average reward: 34.36\n",
      "Training iteration: 341, Average reward: 34.02\n",
      "Training iteration: 342, Average reward: 35.28\n",
      "Training iteration: 343, Average reward: 36.09\n",
      "Training iteration: 344, Average reward: 37.21\n",
      "Training iteration: 345, Average reward: 34.44\n",
      "Training iteration: 346, Average reward: 30.87\n",
      "Training iteration: 347, Average reward: 28.06\n",
      "Training iteration: 348, Average reward: 29.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 349, Average reward: 36.22\n",
      "Training iteration: 350, Average reward: 34.2\n",
      "Training iteration: 351, Average reward: 37.84\n",
      "Training iteration: 352, Average reward: 39.16\n",
      "Training iteration: 353, Average reward: 34.83\n",
      "Training iteration: 354, Average reward: 33.52\n",
      "Training iteration: 355, Average reward: 34.31\n",
      "Training iteration: 356, Average reward: 34.6\n",
      "Training iteration: 357, Average reward: 41.7\n",
      "Training iteration: 358, Average reward: 39.38\n",
      "Training iteration: 359, Average reward: 44.26\n",
      "Training iteration: 360, Average reward: 45.05\n",
      "Training iteration: 361, Average reward: 42.58\n",
      "Training iteration: 362, Average reward: 32.82\n",
      "Training iteration: 363, Average reward: 27.64\n",
      "Training iteration: 364, Average reward: 21.85\n",
      "Training iteration: 365, Average reward: 21.79\n",
      "Training iteration: 366, Average reward: 21.78\n",
      "Training iteration: 367, Average reward: 23.78\n",
      "Training iteration: 368, Average reward: 24.86\n",
      "Training iteration: 369, Average reward: 28.62\n",
      "Training iteration: 370, Average reward: 29.2\n",
      "Training iteration: 371, Average reward: 25.45\n",
      "Training iteration: 372, Average reward: 24.93\n",
      "Training iteration: 373, Average reward: 23.82\n",
      "Training iteration: 374, Average reward: 24.84\n",
      "Training iteration: 375, Average reward: 24.6\n",
      "Training iteration: 376, Average reward: 27.49\n",
      "Training iteration: 377, Average reward: 30.18\n",
      "Training iteration: 378, Average reward: 27.45\n",
      "Training iteration: 379, Average reward: 27.57\n",
      "Training iteration: 380, Average reward: 28.89\n",
      "Training iteration: 381, Average reward: 30.21\n",
      "Training iteration: 382, Average reward: 31.87\n",
      "Training iteration: 383, Average reward: 39.45\n",
      "Training iteration: 384, Average reward: 45.09\n",
      "Training iteration: 385, Average reward: 45.24\n",
      "Training iteration: 386, Average reward: 46.76\n",
      "Training iteration: 387, Average reward: 46.44\n",
      "Training iteration: 388, Average reward: 38.03\n",
      "Training iteration: 389, Average reward: 36.56\n",
      "Training iteration: 390, Average reward: 35.63\n",
      "Training iteration: 391, Average reward: 44.11\n",
      "Training iteration: 392, Average reward: 41.92\n",
      "Training iteration: 393, Average reward: 43.72\n",
      "Training iteration: 394, Average reward: 45.38\n",
      "Training iteration: 395, Average reward: 42.48\n",
      "Training iteration: 396, Average reward: 43.66\n",
      "Training iteration: 397, Average reward: 42.81\n",
      "Training iteration: 398, Average reward: 46.6\n",
      "Training iteration: 399, Average reward: 48.65\n",
      "Training iteration: 400, Average reward: 48.44\n",
      "checkpoint saved at /home/andranik/ray_results/DQN_tron_single_player_2020-03-09_23-55-54_wowtdvl/checkpoint_401/checkpoint-401\n",
      "Training iteration: 401, Average reward: 48.17\n",
      "Training iteration: 402, Average reward: 45.34\n",
      "Training iteration: 403, Average reward: 40.48\n",
      "Training iteration: 404, Average reward: 42.94\n",
      "Training iteration: 405, Average reward: 44.77\n",
      "Training iteration: 406, Average reward: 46.76\n",
      "Training iteration: 407, Average reward: 47.23\n",
      "Training iteration: 408, Average reward: 46.01\n",
      "Training iteration: 409, Average reward: 45.5\n",
      "Training iteration: 410, Average reward: 43.62\n",
      "Training iteration: 411, Average reward: 48.5\n",
      "Training iteration: 412, Average reward: 48.49\n",
      "Training iteration: 413, Average reward: 54.77\n",
      "Training iteration: 414, Average reward: 56.32\n",
      "Training iteration: 415, Average reward: 56.95\n",
      "Training iteration: 416, Average reward: 55.73\n",
      "Training iteration: 417, Average reward: 53.59\n",
      "Training iteration: 418, Average reward: 46.17\n",
      "Training iteration: 419, Average reward: 40.31\n",
      "Training iteration: 420, Average reward: 37.67\n",
      "Training iteration: 421, Average reward: 38.58\n",
      "Training iteration: 422, Average reward: 38.67\n",
      "Training iteration: 423, Average reward: 36.93\n",
      "Training iteration: 424, Average reward: 43.22\n",
      "Training iteration: 425, Average reward: 50.05\n",
      "Training iteration: 426, Average reward: 51.43\n",
      "Training iteration: 427, Average reward: 46.61\n",
      "Training iteration: 428, Average reward: 45.65\n",
      "Training iteration: 429, Average reward: 40.68\n",
      "Training iteration: 430, Average reward: 30.99\n",
      "Training iteration: 431, Average reward: 28.11\n",
      "Training iteration: 432, Average reward: 30.08\n",
      "Training iteration: 433, Average reward: 30.12\n",
      "Training iteration: 434, Average reward: 29.41\n",
      "Training iteration: 435, Average reward: 29.47\n",
      "Training iteration: 436, Average reward: 30.02\n",
      "Training iteration: 437, Average reward: 31.1\n",
      "Training iteration: 438, Average reward: 32.99\n",
      "Training iteration: 439, Average reward: 36.5\n",
      "Training iteration: 440, Average reward: 45.54\n",
      "Training iteration: 441, Average reward: 47.92\n",
      "Training iteration: 442, Average reward: 48.46\n",
      "Training iteration: 443, Average reward: 48.27\n",
      "Training iteration: 444, Average reward: 49.28\n",
      "Training iteration: 445, Average reward: 43.34\n",
      "Training iteration: 446, Average reward: 45.06\n",
      "Training iteration: 447, Average reward: 43.56\n",
      "Training iteration: 448, Average reward: 40.39\n",
      "Training iteration: 449, Average reward: 40.02\n",
      "Training iteration: 450, Average reward: 45.82\n",
      "Training iteration: 451, Average reward: 48.59\n",
      "Training iteration: 452, Average reward: 49.2\n",
      "Training iteration: 453, Average reward: 51.74\n",
      "Training iteration: 454, Average reward: 46.38\n",
      "Training iteration: 455, Average reward: 42.57\n",
      "Training iteration: 456, Average reward: 46.22\n",
      "Training iteration: 457, Average reward: 46.49\n",
      "Training iteration: 458, Average reward: 45.41\n",
      "Training iteration: 459, Average reward: 42.75\n",
      "Training iteration: 460, Average reward: 41.49\n",
      "Training iteration: 461, Average reward: 42.48\n",
      "Training iteration: 462, Average reward: 38.33\n",
      "Training iteration: 463, Average reward: 40.45\n",
      "Training iteration: 464, Average reward: 42.27\n",
      "Training iteration: 465, Average reward: 45.8\n",
      "Training iteration: 466, Average reward: 45.5\n",
      "Training iteration: 467, Average reward: 51.69\n",
      "Training iteration: 468"
     ]
    }
   ],
   "source": [
    "# Initialize training environment\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "def environment_creater(params=None):\n",
    "    agent = SimpleAvoidAgent(noise=0.05)\n",
    "    return SinglePlayer(TronRllibEnvironment(board_size=13, num_players=4), agents=agent)\n",
    "\n",
    "env = environment_creater()\n",
    "tune.register_env(\"tron_single_player\", environment_creater)\n",
    "ModelCatalog.register_custom_preprocessor(\"tron_prep\", TronExtractBoard)\n",
    "\n",
    "# Configure Deep Q Learning with reasonable values\n",
    "# config = DEFAULT_CONFIG.copy()\n",
    "# config['num_workers'] = 4\n",
    "# config['num_gpus'] = 0\n",
    "# config[\"timesteps_per_iteration\"] = 1024\n",
    "# config['target_network_update_freq'] = 2048\n",
    "# config['buffer_size'] = 50_000\n",
    "# config['schedule_max_timesteps'] = 200_000\n",
    "# config['exploration_fraction'] = 0.9\n",
    "# config['compress_observations'] = False\n",
    "# config['num_envs_per_worker'] = 1 if LOAD_FROM_CHECKPOINT else 4\n",
    "# config['train_batch_size'] = 4096\n",
    "# config['n_step'] = 2\n",
    "# config['seed'] = SEED\n",
    "\n",
    "# Configure Deep Q Learning for multi-agent training\n",
    "config = DEFAULT_CONFIG.copy()\n",
    "config['num_workers'] = 4\n",
    "config[\"timesteps_per_iteration\"] = 128\n",
    "config['target_network_update_freq'] = 256\n",
    "config['buffer_size'] = 10_000\n",
    "config['schedule_max_timesteps'] = 100_000\n",
    "config['exploration_fraction'] = 0.9\n",
    "config['compress_observations'] = False\n",
    "config['num_envs_per_worker'] = 1 if LOAD_FROM_CHECKPOINT else 4\n",
    "config['train_batch_size'] = 256\n",
    "config['n_step'] = 2\n",
    "config['seed'] = SEED\n",
    "\n",
    "# We will use a simple convolution network with 3 layers as our feature extractor\n",
    "config['model']['vf_share_layers'] = True\n",
    "config['model']['conv_filters'] = [(64, 5, 2), (128, 3, 2), (256, 3, 2)]\n",
    "config['model']['fcnet_hiddens'] = [256]\n",
    "config['model']['custom_preprocessor'] = 'tron_prep'\n",
    "\n",
    "# Begin training or evaluation\n",
    "trainer = DQNTrainer(config, \"tron_single_player\")\n",
    "render = TronRender(13, 4)\n",
    "\n",
    "if LOAD_FROM_CHECKPOINT:\n",
    "    np.random.seed(SEED)\n",
    "    trainer.restore(\"/home/andranik/ray_results/DQN_tron_single_player_2020-03-09_17-37-30tbji4p81/checkpoint_101/checkpoint-101\")\n",
    "    for _ in range(10):\n",
    "        print(test(render, env, trainer))\n",
    "        sleep(3)\n",
    "else:\n",
    "    num_epoch = 5001\n",
    "    test_epochs = 100\n",
    "    for epoch in range(num_epoch):\n",
    "        print(\"Training iteration: {}\".format(epoch), end='')\n",
    "        res = trainer.train()\n",
    "        print(f\", Average reward: {res['episode_reward_mean']}\")\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            checkpoint = trainer.save()\n",
    "            print(\"checkpoint saved at\", checkpoint)\n",
    "\n",
    "        if epoch % test_epochs == 0:\n",
    "            for _ in range(3):\n",
    "                reward = test(render, env, trainer)\n",
    "                sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "trainer.restore(checkpoint)\n",
    "for _ in range(10):\n",
    "    print(test(render, env, trainer))\n",
    "    sleep(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (colosseumrl)",
   "language": "python",
   "name": "colosseumrl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
